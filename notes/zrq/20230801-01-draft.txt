#
# <meta:header>
#   <meta:licence>
#     Copyright (C) 2023 by Wizzard Solutions Ltd, wizzard@metagrid.co.uk
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
# AIMetrics: []
#

    Target:

        Initial draft

    Result:

        Work in progress ...

# -----------------------------------------------------

* Introduction

The ExecutionPlanner specification defines the ExecutionPlanner and ExecutionWorker web-services, and a common data model
for describing executable tasks.
Designed to provide a common interface for service discovery, resource allocation and asynchronous execution of executable tasks
across a heterogeneous federation of different execution platforms.

* ExecutionPlanner service – a discovery service to find execution platforms, allocate resources and schedule execution
* ExecutionWorker service – an asynchronous service for executing a specific task (UWS pattern)
* ExecutionPlanner data model – common data model for describing executable tasks and resource requirements.

** Common terms

Task, job, and workflow, can have different and sometimes overlapping meanings.
This document is based on the following definitions :

Task - An executable component e.g. a Python program or a JupyterNotebook and a description of the resources needed to execute it.
Job - The assignment of a task to a worker.
Job record - An entry in a database tracking the status of job as it is executed.
Workflow - A document describing a sequence of tasks, which could itself, be considered as an executable task.

If we start by thinking of a science domain function that we want to perform.
For example, the mathematical concept of the square root of a number.

We can calculate the square root of a number using a numerical analysis
algorithm that makes successive approximations of the result.

We can write a Python program to use this algorithm to calculate the square root of a number.
This is the first identifiable 'executable thing' in our example.

To be able to use this 'executable thing' you would need to allocate a computing resource with the appropriate
hardware and software environment. In this case, a computing resource with the Python interpreter installed
along with any 3rd Python modules equired by the program. This environment is often referred to as a 'Python runtime'.

In the context of a science platform, a common pattern is to provide this environment using an OCI container
to package a basic operating system, the Python interpreter, and the Python program, as a single binary object.

This package, or container, is itself an 'executable thing'. One which requires a different execution
environment than the bare Python program. The key concept behind the OCI container pattern is
to package programs as binary objects that all use a common execution environment, commonly referred
to as the container runtime or container engine.

To be able to use this 'executable thing' you would need to allocate a computing resource with the appropriate
hardware and software environment. In this case, a computing resource with the OCI container runtime installed.

We could also create a Jupyter notebook that demonstrates how to use our Python program.
This is the third 'executable thing' in our example. One which provides an interactive environment
for the user to experiment with.

As before, to be able to use this 'executable thing' we would need to allocate a computing resource with
the appropriate hardware and software environment.
In this case, a computer with the Jupyter notebook platform installed along with specifc set of 3rd Python modules.
In the context of a science platform, a common pattern is to provide this environment as a web-service
with a HTTP web interface that allows the user to connect to the Jupyter notebook via a web browser.

From one algorithm that implements a science domain function, we have create three different 'executable things'.
A Python program, an OCI container packaging the Python program and its dependencies, and a Jupyter notebook
that demonstrates how to use the Python progran.

Each of these 'executable things' requires a different computing environment to execute.
A basic Python runtime, the OCI container runtime, and a Jupyter notebook service.

We may also want to consider the data that we are applying the algorithm to.
If we are learning how to use algorithm, then a basic computing resource would be sufficient
to experiment with.
However, if we have a data set of ten million numbers that we want to process, then we may
need to consider adding extra storage to store our input data and our results.

It may also be worth using a GPU to accelerate the calculation steps for such a large data set.

In terms of the ExecutionPlanner, a task description describes an 'executable thing',
like our OCI container or our Jupyter notebook, and the resources needed to execute it.
The number of cpu cores and amount of memory it needs, whether it needs a GPU,
the location of the input data, the storage space needed to perform the calculation
and the space needed to save the results.

Stepping up a level, we could use a workflow language to map out a sequence of
executable steps that combine things like our square root calculation to perform
a higher level science goal.

In the context of the ExecutionPlanner, a workflow described by a workflow language is
functionally equivalent to a program described in a programming language.

In terms of service discovery, resource allocation and execution scheduling,
asking 'can you run this workflow' is functionally equivalent to asking 'can you run this program.'
They both require computing resources with the appropriate software and hardware installed.



* Conversation

The core idea behind the ExecutionPlanning specifications is based on a conversation between the client and one or more ExecutionPlanner
services to describe a task and the resources needed to execute it and to discover where, how, and when, it can be executed.

The conversation starts with the client sending a question describing the task it wants executed to one or more ExecutionPlanner
services, which responds with a top level YES/NO answer, and if the answer is YES, one or more offers describing where how, and when,
the task could be executed.

The client chooses which of the offers it wants to use and sends sends a message back to that ExecutionPlanner service accepting the offer.
The ExecutionPlanner will then cancel the other offers it made and reserves the resources described by the accepted the offer.

The client then uses the connection details in the offer to send the request to the appropriate ExecutionWorker service which will
execute the job at the appropriate time.

The client does not need have to cancel the offers made by the other ExecutionPlanner services. Offers are only valid for a short period
of time and expire naturally when they reach the end of their lifetime.

* Core questions

The task description part of the conversation is best explained thinking of it as a series of questions
that can be used to discover where how, and when, to execute a task.

**
Level 1 - the executable task
Can this platform execute this <task> ?

At the simplest level we just want to check whether the platform is able to execute a particular type of task.
For example, _is this platform able to run a Jupyter notebook?_

In order to do this the request needs to specify the task type, e.g. Jupyter notebook,
along with details about the task, e.g. where to fetch it from and how to execute it.

The ExecutionPlanner response would be YES or NO, followed by a list of offers for when and where the task could be executed.

In order to keep the specification open ended, each particular type of task would be described in a separate extension to the core ExecutionPlanningDataModel.
The core data model has two fields, a URI identifying the task type, and place holder for details about the task.

```
    task:
      # A URI identifying the basic type of task.
      # e.g.
      # https://www.purl.org/ivoa-net/task-type/oci-container
      # https://www.purl.org/ivoa-net/task-type/jupyter-notebook
      type: "any-URI"

      # Task details, specific to the task-type.
      spec: {}
```

The core ExecutionPlanning specification defines an initial set of the most common task types, e.g. OCI container, Jupyter notebook etc.
Then the plug-in extension pattern makes it easy for 3rd parties to develop new types of tasks appropriate for their science domain.

A service that doesn’t recognize a particular task type can simply reply NO to an execution request.

```
Request  - Can this platform execute <unkown-task-type> ?
Response - NO
```

The data model extension for a task type would define the meta-data needed to describe that particular type of task.

For example, the task description for executing a Jupyter notebook would describe where to fetch the source code for the notebook from.
```
    task:
      # The URI for the task type
      type: "https://.../jupyter-notebook"

      # Task details, specific to the Jupyter notebook task type.
      spec:
        notebook: "https://.../example.jpnb"
```

The task description for executing an OCI container would describe what system architecture the container is is built for
and where to fetch the binary image from.

```
    task:
      # The URI for the task type
      type: "https://.../oci-container"

      # Task details, specific to the OCI-container task-type.
      spec:
        arch:  "..."
        repo:  "..."
        image: "..."
```

Details of the core task types is defined in section xxx.

**
Level 2 - resources
Can this platform provide the <resources> needed to execute this <task> ?

This level checks that the execution platform can provide the resources needed to execute the task.

***
Level 2a - compute resources

In order to do this the request would not only describe the task itself,
but also the minimum level of compute resources in terms of gpus, cpu cores, memory, and disc space etc,
needed to execute it.

The metadata for describing compute resources is, in most cases, common to all types of task,
so the data model for these requirements are defined as part of the core ExecutionPlanning data model.

It is important to note that all of the resource requirements are optional.
So a request to execute a simple Jupyter notebook would not need to include any resource details.

```
    task:
      type: "https://.../jupyter-notebook"
      spec:
        notebook:  "https://.../example.jpnb"
```

As long as this Jupyter notebook only needs a minimal set of resources to run, e.g. 2 cpu cores,
2G of memory and 20G of disc space, then this task will execute pretty much anywere.

However, if the Jupyter notebook needs a specific type of GPU to function properly,
then this needs to be added to the request by specifying a compute environment that
provides that specific type of GPU.

```
    task:
      type: "https://.../jupyter-notebook"
      spec:
        notebook: "https://.../example.jpnb"
    resources:
      compute:
      - name: "example-one"
        gpu:
          type: "https://.../AMD-xx"
```

With this detail added to the request, platforms that are not able to provide this kind of GPU would simply reply NO.

```
Request  - Can this platform provide a machine with an AMD-xx GPU to execute a jupyter-notebook ?
Response - NO
```

Note that a platform does not need to know what an "AMD-xx GPU" is to be able to reply with a sensible aswer.
If a platform receives a request for resource requirement that they don't understand, they should simply reply NO.

The only platforms that reply YES are ones that understand what an AMD-xx GPU is and are able to provide
a compute resource with one.

```
Request  - Can this platform provide a compute resource with an AMD-xx GPU to execute a jupyter-notebook ?
Response - YES
```

The data model for describing compoute resources also includes elements for specifying the size and number of
resources such as cpu cores, memory and storage.

If the Jupyter notebook needs a minimum of 8 cpu cores and 16G bytes of memory to be able to perform its calculations,
then this can be added to the required compute resources.

```
    task:
      type: "https://.../jupyter-notebook"
      spec:
        notebook: "https://.../example.jpnb"
    resources:
      compute:
      - name: "example-one"
        min-cores:   8
        min-memory: 16Gb
        gpu:
          type: "https://.../AMD-xx"

```

All of the elements for specifying the size or number of compute resources are defined in the data model as pairs of minimum and maximum values.
This allows a conversation to occur between the ExecutionPlanner and the client to discover the best platform to execute the task on.

The client requests the minimun resources it needs, and the service may respond with an offer to provide a higher level of resources if they are available.

For example, if a platform is able to provide double the compute resources, 16G of memory and 32 cores,
then it may indicate this by specifying higher maximum values in its response.

```
    ....
    resources:
      compute:
      - name: "example-one"
        min-cores:   8
        max-cores:  16
        min-memory: 16Gb
        max-memory: 32Gb
        gpu:
          type: "https://.../AMD-xx"
```

This response represents an offer to start with a minimum of 8 cores and 16G of memory as reqiuested,
with the option to use a maximum of 16 cores and 32G of memory if needed.

This is slightly different to a case where a platform like Openstack which allocates resources
in specific blocks, defined by the set by the set of virtual machine 'flavors' available on that
paerticular platform.
If the smallest flavor of virtual machine available on the platform has 16 cores and 24G of memory,
then the service can represent that by making an offer with higher minimum values.

```
    ....
    resources:
      compute:
      - name: "example-one"
        min-cores:  16
        max-cores:  16
        min-memory: 24Gb
        max-memory: 32Gb
        gpu:
          type: "https://.../AMD-xx"
```

This response represents an offer to start with a minimum allocation of 16 cores and 24G of memory,
with the oprion to use up to a maximum 32G of memory if needed.

An executionPlanner MAY NOT make an offer with less than the minimum resources requested.
For example, if an Openstack platform only has a tiny virtual machine flavor available,
with 2 cpu cores and only 2G of memory, then it MAY NOT make an offer with less than
the requested minimums.
```
    ....
    resources:
      compute:
      - name: "example-one"
        min-cores:   8
        max-cores:   2
        min-memory: 16Gb
        max-memory:  2Gb
        gpu:
          type: "https://.../AMD-xx"
```

Although it is technically possible to resize virtual machines,
these minimum and maximum values are more applicable to something like a Kubernetes cluster
where the execution can start with a minimum configuration and then scale on demand
up to a maximum limit.

Note that the term 'compute resource' specifically avoids stating whether the the
notebook is run in a virtual machine or a Kubernetes cluster.
As far as the user is concerned, it doesn't matter, as long as the compute resource provides
sufficient cpu cores and memory for the noteboot to execute.

If a particular task does require a specific type of compute resource,
for example a Helm chart that needs a particular version of Kubernetes,
then then this can be added to the compute resource requirements.

```
    ....
    task:
      type: "https://.../helm-chart"
      spec:
        ....
    resources:
      compute:
      - name: "example-two"
        type: "https://.../kubernetes"
        kubernetes:
          version: "1.25.4"
        ....
```

Full details of the data mode for decribing compute resources is given in section xx.

***
Level 2b - storage resources

The resources section of the request can also be used to specify storage resources.

There are two main types of storage resources.
* External storage resources that are managed by an external provider.
* Internal storage resources that are managed by the execution platform.

Within that there are two main types of internal storage resources:
* Ephemeral storage resources managed by the system are available for the duration of the job, and released as soon as the job is completed.
* Persistent storage resources managed by the system exist beyond the lifetime of the job, this type of storage is created before the job starts and remains after the job has been completed.

The simplest to describe are the ephemeral storage resources managed by the execution platform.
For example, if the selected Jupyter notebook requires 1Tbyte of scratch space to perform its calculations,
then this can be added to the request by defining an ephemeral storae resource.

```
    task:
      type: "https://.../jupyter-notebook"
      spec:
        notebook: "https://.../example.jpnb"
    resources:
      ....
      storage:
      - name: "scratch-space"
        type: "https://.../ephemeral"
        size: 1024Gb
```

To define how the Jupyter notebook can access this storage we need to add a corresponding volume element to the compute resource
that describes where to mount the storage volume.

```
    task:
      type: "https://.../jupyter-notebook"
      spec:
        notebook: "https://.../example.jpnb"
    resources:
      compute:
      - name: "example-one"
        ....
        volumes:
        - name: "scratch-space"
          path: "/temp"
          mode: "rw"
      storage:
      - name: "scratch-space"
        type: "https://.../ephemeral"
        size: 1024Gb
```

This part of the ExecutionPlanning data model, separating the details of how a storage resource is implemenmted from the details of
how is mounted inside a computing resource, is based on the pattern used by Kubernetes to describe volumes and their mount points within containers.

This pattern can also be used to define a storage resource that imports data from an external source.
For example, if the user wanted to use the notebook to analyse data stored in an external S3 system,
this can be done using a storage resource to describes where the data is and how to access it,

```
    ....
    resources:
      ....
      storage:
      - name: "source-data"
        type: "https://.../amazon-S3"
        endpoint: "https://..."
        bucket: "example-data"
```

and a corresponding volume mount to describe how to make the resource assessible from inside the compute resource.
```
    ....
    resources:
      compute:
      - name: "example-one"
        ....
        volumes:
        - name: "source-data"
          path: "/data"
          mode: "r"
```

Again, this pattern of separating how the data is stored outside the system and how it appears inside the compute resource
borrows heavily from the pattern used by Kubernetes to describe Persistent Volumes (PV) and Persistet Volume Claims (PVC).

The same pattern can be used to describe a persistent storage resource that can be used to save the analysis results,
by defining a persistent storage resource that will be allocated by the system
```
    ....
    resources:
      ....
      storage:
      - name: "result-output"
        type: "https://.../persistent"
        lifetime: "1D"
        size: 100Gb
```

and a corresponding volume mount to describe how to make the storage resource assessible inside the compute resource.
```
    ....
    resources:
      compute:
      - name: "example-one"
        ....
        volumes:
        - name: "result-output"
          path: "/results"
          mode: "rw"
```

By setting the storage resource type to "https://.../persistent", the client is asking the ExecutionPlanner service
to take care of allocating the storage and manage its lifecycle.

For example, the execution platform may have a Rucio storage system which it uses for user generated data.
In which case the service would respond with an offer for storing the
resource in the Rucio system along with details of how the client can
access the it.

```
    ....
    resources:
      ....
      storage:
      - name: "result-output"
        type: "https://.../rucio"
        endpoint: "http://...."
        objectid: "cdc78e7d-8032-497e-9a5b-01c720ea2223"
        lifecycle: "managed"
        lifetime: "1D"
        size: 100Gb
```
making an offer with a the lifecycle set to "managed" and a lifetime of "1D"
means that the service will manage the lifecycle automatically
and the resource will be avalable for a day after the job completes.

It is important to note that ther storage is proposed, but not yet allocated.
The persistent storage is only allocated if the client accepts this particular offer.
This allows a service to make multiple offers with different options for the persistent storage,
allowing the client to select and accept the one that best fits its use case.

See section xx for more details about the time sequence for making and accepting offers.

**
Level 3 - user identity
Can this platform provide <identity> with <resources> and <environment> to execute this <task> ?

This level checks that the user account making the request has sufficient access rights and resource quota to perform the task.

This level uses the same pattern of plugin extensions to handle different authentication methods.
The core data model for the request and response defines two elements, a URI that identifies the type of authentication method, and
and a place holder for additional details for that authentication method.

```
    auth:
      # A URI identifying the authentication method.
      # e.g.
      # https://www.purl.org/ivoa-net/auth-type/....
      # https://www.purl.org/ivoa-net/auth-type/....
      type: "any-URI"

      # Additional details, specific to the authentication method.
      spec: {}
```

If the authentication method is one of the methods defined by the IVOA SSO specification,
then all that is required is the URI to indicate which of the IVOA SSO methods
is being used for this request.
The rest of the authentication information is supplied in the HTTP headers,
as described in the IVOA SSO specification.

However, this pattern also allows an ExecutionPlanner instance to accept
an authentication method that is not covered by the IVOA SSO specification.

This means that the ExecutionPlanner web service can be deployed in other domains,
outside the IVOA, without requiring the domain to adopt the IVOA SSO specification.

A client is free to use any authentication method, including one not defined or endorsed
by the IVOA SSO specification. It is up to the service to decide how it handles the
authentication infomation supplied.

If the ExecutionPlanner service does not understand the authentication method it can
choose one of three options for handling the response.
* It MAY simply ignore the authentication and treat the request as an anonymous request.
* It MAY accept the HTTP request by reject the web-service request, responding with a valid REST response saying "NO,
this service cannot run the task because the requested authentication method is not supported",
* It MAY reject the HTTP request and return a 401 Unauthorized response.

Including details of the authentication method in the response offers allows the service to
communicate to the client which authentication method was accepted.

In the case of a service downgrading or ignoring an authentication method that is doesn't understand,
the service would indicate this by responding with an offer specifying an anonymous request with no authentication.

----

Extension - allow more than one auth method in the same request ?
A client may supply more than one authentication method in a request. For example by
providing the header fields for a BASIC username/password authentication and an x509 certificate
in the same request.
In this case, the service may respond with three offers,
one for an anonymous request with no authentication,
one for the BASIC username/password authentication,
and one for the x509 certificate.

----

Note that an IVOA Registry entry for an ExecutionPlanner service MAY include details of the authentication
methods the ExecutionPlanner service accepts, as defined in the IVOA SSO specification. However it is not required.

The ExecutionPlanner specification is designed to work alongside the other IVOA specificationms,
but it does not require them to function.

----

**
Level 4 - date and time
<when> can this platform provide <identity> with <resources> and <environment> to execute this <task> ?

This level enables the client and server to have a conversation about when the task would be executed.

The client can specify a range of times when it would like to start the task, and the minimum duration
needed to complete the task.

The service can respond with one more offers that specify when the task will start and the maximum duration
that the task will be allowed to consume.
It is then up to the client to select which of the offers best suites the use case.

Start times in request and response are expressed as an array of time intervals, as defined by ISO 8601.
Specifically, type 1, 2 or 3 intervals, excluding type 4 intervals (duration only) and excluding repeats.
* <start>/<end>
* <start>/<duration>
* <duration>/<end>

Note that the duration part of the interval applies to the start time, specifying the
range of allowed start times.

If no duration is specified, then this means an absolute start time, e,g, 11:30.
```
time:
- start: "2023-09-14T11:30Z"
```

If an end is specified, then this means the start time may be somewhere between the start and end times, e,g, between 11:30 and 12:00.
```
time:
- start: "2023-09-14T11:30Z/T12:00Z"
```

If a duration is specified, then this means the start time may be somewhere between the start and the start plus the duration, e,g, between 11:30 and 12:00.
```
time:
- start: "2023-09-14T11:30Z/PT30M"
```

The maximum and minimum execution duration are expressed as time periods, as defined by ISO 8601.

For example, for the unattended batch mode execution of an OCI container, the user might not be concerned about
when the task starts, but they amy want to specify the minimum duration needed to complete the task.

In which case the client simply may request a minimum duration of one hour.
```
time:
- minduration: "1H"
```

The service respond with more than one offer that start at different times and set different values for the
maximum duration.

It may offer a maximum duration of one hour starting at 11am on Monday 14th.
```
time:
- start: "2023-09-14T11:30Z"
  minduration: "1H"
  maxduration: "1H"
```

It may also offer two hours execution starting between 10pm and 11pm.
```
time:
- start: "2023-09-14T22:00Z/PT1H"
  minduration: "P1H"
  maxduration: "P2H"
```

If all of the other parameters are the same, compute and storage resources, then the
service may offer more than one time slot in the same offer.

```
time:
- start: "2023-09-14T11:30Z"
  minduration: "1H"
  maxduration: "1H"
- start: "2023-09-14T22:00Z/PT1H"
  minduration: "P1H"
  maxduration: "P2H"
```

Alternatively, if the compute resources are different, then the service would have to include
two offers in the response.

For example, if the client asks for 2 cores, 2Gb of memory for 1 hour sometime on Monday the 14th:
- task:
  ...
  resources:
    compute:
      mincores: 2
      minmemory: 2Gb
  time:
  - start: "2023-09-14/P1D"
    minduration: "P1H"
```

The service can respond with 2 offers, the minimum
2 cores and 2Gb of memory for 1 hour starting at
11:30am, or 8 cores and 8Gb of memory for 4 hours starting
sometime between 10pm and 11pm.

```
offers:
- task:
  ...
  resources:
    compute:
      mincores: 2
      maxcores: 2
      minmemory: 2Gb
      maxmemory: 2Gb
  time:
  - start: "2023-09-14T11:30Z"
    minduration: "P1H"
    maxduration: "P1H"

- task:
  ...
  resources:
    compute:
      mincores: 2
      maxcores: 8
      minmemory: 2Gb
      maxmemory: 8Gb
  time:
  - start: "2023-09-14T22:00Z/PT1H"
    minduration: "P1H"
    maxduration: "P4H"
```

It is then up to the client to decide which offer better suites their use case.
Accept the limited offer in the morning, or accept the later offer with more resources and more time.

*
Use cases

** General requirements.

Operate alongside, but not require, the other IVOA specifications.

* The ExecutionPlanner specification should not include any astronomy or IVOA specific elements.
* An ExecutionPlanner services MAY be registered in the IVOA registry, but it is not required.
* An ExecutionPlanner services MAY follow the IVOA SSO authentication specification, but it may use other authentication methods if it needs to.

This is to lower the barrier to entry and to allow the ExecutionPlanner to be used by other projects in other science domains, outside the scope of the IVOA.

** Client side use cases.

A simple JupyterNotebook.

A JupyterNotebook, with metadata provided by the user.
cpu, memory and gpu

A JupyterNotebook, with metadata provided by the developer.
cpu, memory and gpu
abstract data reference

An OCI container, unattended batch mode execution.
cpu, memory and gpu

An OCI container, interactive command line.
cpu, memory and gpu
startdate
single user

An OCI container, interactive command line.
cpu, memory and gpu
startdate
workshop

A Helm chart, multiple compute resources
Zeppelin and Spark platform
cpu, memory and gpu
startdate
single user

A Helm chart, multiple compute resources
Zeppelin and Spark platform
cpu, memory and gpu
startdate
workshop

*
Request and response

Details of the request and response messages.
Timeline of request, offer and accept.

*
ExecutionWorker

Derived from IVOA UWS, using the same data model. Request and response serialization TBD.
Accepting an offer starts a job on an ExecutionWorker, status PENDING until the job is started automatically at the start time given in the offer.
ExecutionPlanner accept response contains the Job URL at the ExecutionWorker.
User can cancel the Job anytime up until the end.

*
Architecture

The ExecutionPlanner and ExecutionWorker service specifications are designed to be used at multiple levels
within an organization.

** Project level use cases

At the low level, an ExecutionPlanner and ExecutionWorker pair may be implemented as a
single web-application linked to a simple Docker execution service.
The configuration may be hard coded to only accept a fixed white list of container images
and a fixed allocation of compute resources for each job.

e.g. container images xxxx and yyyy, 2 cpu cores, 2G of memory, max 1HR duration.

A project or organization may deploy several of these low level services,
providing a range of different capabilities.

Given a new task to execute a client can poll each of the services to see which one will
make an offer to execute it.

The client does not need to have any prior knowledge about the services apart from their
endpoint address and perhaps the version of the API that they implement.
This information could come from an IVOA Registry instance, or it could simply
be provided in a configuration file for the client.

A more flexible architecture could add another ExecutionPlanner service
a level above the low level task specific services.
This service would be configured with the list of local task specific services
and act as an aggregator service sitting between the client and the low level services.
This aggregator service would forward a copy of each request it receives to each of the low level services and
then aggregate the offers from the individual responses into a single response that is sent back to the client.

A simple aggregator service would just forward every request to all the low level services below it,
regardless of what the request contained.

A more complex aggregator service may have some prior knowledge about what types of task or compute resources
each of the lower level services were able to offer, enabling it to make more informed decisions about
which low level serviecs to send the requests to.

Another aggregator service have an understanding of the location of data sets within the organization and
be able to route requests for tasks to different low level services depending on which data sets the tasks required.

The aggregator services may expose the low level UWS ExecutionWorker endpoints in its responses,
or it may also implement an ExecutionWorker interface that acts as a proxy for the low level
ExecutionWorker services.

This configuration could be used to provide an ExecutionPlanner and ExecutionWorker service interface
that bridged a firewall. Providing a public interface for external clients and forwarding the requests
to internal ExecutionPlanner and ExecutionWorker services that are not accessible from outside the firewall.

A large organization with multiple sites may deploy a single high level ExecutionPlanner/ExecutionWorker
interface that handles task execution for the whole organization, forwarding the requests to a mid-level
ExecutionPlanner/ExecutionWorker at each sitem which in torn forwards the requests to individual low-level
task specific ExecutionPlanner/ExecutionWorker services within the sites.

The implementation at each level may be different, providing different levels of routing
and aggregation based on internal knowledge of the capabilities of the level below,
but the ExecutionPlanner and ExecutionWorker interfaces are the same at every level
in the organization.

This could be expanded to include another level of ExecutionPlanner/ExecutionWorker services that
cross organization bondaries, allowing users to access services from different projects and organizations.

Example:

SKA,
regional centers
local data centers

LSST,
regional centers
local data centers

CTA,
regional centers
local data centers

CERN,
regional centers
local data centers

* Known unknowns

**
Data location

At the moment, we don't have a good way to describe the location of a computing or data resource.
Nor do we have a way of calculating the proximity in time and space bnetween them.

    Group based - is a member of <data center north>
    Time based - would take <20min to transfer 10Gbytes>

At the moment we don't have a good vocabulary for describing data access speeds.
If a data resource is far away from a compure resource, then access speed will be slow.
The closer they are the faster the access speed will be.

Moving the data to the code can be an expensive operation.
It takes time, bandwidth and in some cases money to move data between locations.

Moving the code to the data can be an expensive operation.
A service provider closer to the data may have less compute resources.

We need a vocabulary to be able to describe location, proximity and access speed
to be able to talk about choices between data and compute resource locations.













